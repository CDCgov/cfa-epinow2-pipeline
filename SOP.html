<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NA • CFAEpiNow2Pipeline</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="NA"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">CFAEpiNow2Pipeline</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>NA</h1>

    </div>


<div class="section level2">
<h2 id="epinow2-rt-pipeline">Epinow2 Rt Pipeline<a class="anchor" aria-label="anchor" href="#epinow2-rt-pipeline"></a></h2>
<p>This document is a guide to running the weekly Rt estimation pipeline from within the VAP (Virtual Analyst Platform). The main command to run the weekly pipeline (found in the Makefile) is <code>make run-prod</code>. Running this will create and utilize a suite of configuration files (.json) specified from within the associated blob storage account and will produce outputs in the <code>rt-epinow2-output</code> Azure blob storage account.</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<div class="section level3">
<h3 id="the-whole-workflow">The whole workflow<a class="anchor" aria-label="anchor" href="#the-whole-workflow"></a></h3>
<p>(For a production run on main)</p>
<pre class="mermaid"><code>flowchart TD

   subgraph Azure Batch pools
      subgraph cfa-epinow2-latest
           myjob("set up job") --&gt;
           run("run epinow2") --&gt;
           write("write outputs")
      end
   end


   subgraph Azure blob storage
     azc("
       **configs**
       az://rt-epinow2-config/
       ")
     outprod("
       **production outputs**
       az://nssp-rt-v2
       ")
     outtest("
       **non-production outputs**
       az://nssp-rt-testing
       ")
     data("
       **data**
       az://nssp-etl/gold/
       ")
   end

   subgraph Azure container registry
     acr_latest("rt-epinow2-config:latest")
   end

  init[
    **configure job**
    with cfa-config-generator
   ] -- generate --&gt;

     configs[/
        job_id/task_1_id.json
        job_id/task_2_id.json
        ...
        job_id/task_n_id.json
     /] -- upload --&gt; azc

   data -- pull --&gt; myjob
   acr_latest -- pull --&gt; myjob
   azc -- pull --&gt; myjob
   write -- production run --&gt; outprod</code></pre>
</div>
<div class="section level3">
<h3 id="definitions">Definitions<a class="anchor" aria-label="anchor" href="#definitions"></a></h3>
<p>A <strong>job</strong> is a unique pipeline run, which consists of a set of configuration files that fully define the run, and a set of outputs. The <code>job_id</code> is a unique tag assigned by the config generator that links the configuration files to the run and the outputs. A job may contain many tasks (e.g. a typical production job contains tasks for each of 51 jurisdictions * 2 diseases = 102 tasks, each with their own <code>task_id</code>). Each task is reproducible and defined by its own .json config file.</p>
</div>
<div class="section level3">
<h3 id="a-full-pipeline-run-consists-of">A full pipeline run consists of<a class="anchor" aria-label="anchor" href="#a-full-pipeline-run-consists-of"></a></h3>
<ol style="list-style-type: decimal"><li><p>Use the <a href="https://github.com/CDCgov/cfa-config-generator" class="external-link">cfa-config-generator</a> to create a new <code>job_id</code>, generate configuration files for your job and upload them to blob.</p></li>
<li>
<p>Set up the job</p>
<ul><li>In Azure, this involves pulling the appropriate container image, configuration files, and data into the appropriate Azure Batch pool. Usually, we use the “cfa-epinow2-pipeline:latest” image and pool, which correspond to the main branch of the cfa-epinow2-pipeline repo. We have set up continuous deployment workflows in this repo so that the image should be pre-loaded in the container registry, and the pool should be idling and ready to run a job without any action from you.</li>
<li>You may also pull these resources down to run a job locally, e.g. using the computing power of your VAP image. But we don’t recommend this unless you are running a small job with few tasks, because your local machine does not provide the same parallelism as Batch.</li>
<li>If you are developing code on a branch in this repo, opening a PR or draft PR will trigger GH actions that will build a pool and container tagged with your branch name, “cfa-epinow2-pipeline:my_branch”, and the makefile in this repo will automatically configure the job to run using the pool and image corresponding to your branch so that you can test your updates in Azure. Note, however, that the pool and container image will contain the latest version of the code pushed to the remote, not the latest version you have locally, and you will need to wait for the pool and image to rebuild in the cloud after each push to GitHub.</li>
</ul></li>
<li><p>Run the job. Once all the required inputs are on a computer, inside the appropriate container image, for each task, the epinow2 model will run, generate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mi>t</mi></msub><annotation encoding="application/x-tex">R_t</annotation></semantics></math> estimates, and write them to the container’s file system.</p></li>
<li><p>Upload the outputs from the container to a persistent storage location, such as Azure Blob (or for local runs, to your local filesystem).</p></li>
</ol><p>Downstream of this pipeline, the cfa-rt-postprocessing repo provides additional tools to parse and evaluate the model’s outputs.</p>
<p>This may seem complicated, but once you have your dependencies and credentials set up, the whole workflow can be run with a single call to <code>make run-prod</code> at the command line. This SOP will walk you through the required setup and the underlying steps.</p>
</div>
<div class="section level3">
<h3 id="storage-and-related-repos">Storage and related repos<a class="anchor" aria-label="anchor" href="#storage-and-related-repos"></a></h3>
<ul><li>
<a href="https://github.com/CDCgov/cfa-config-generator" class="external-link uri">https://github.com/CDCgov/cfa-config-generator</a>: a microservice that generates configuration files, and uploads them to Azure blob storage</li>
<li>
<a href="https://github.com/CDCent/cfa-rt-postprocessing" class="external-link uri">https://github.com/CDCent/cfa-rt-postprocessing</a>: a repo with tools for postprocessing of Rt estimates</li>
<li>
<code>az://rt-epinow2-config/</code> an Azure blob storage container that holds configuration files</li>
<li>
<code>az://nssp-rt-v2/</code> an Azure blob storage container where we store production outputs</li>
<li>
<code>az://nssp-rt-testing/</code> an Azure blob storage container where we store non-production outputs</li>
<li>
<code>az://nssp-rt-post-process/</code> an Azure blob storage container where we store output from the post processor</li>
</ul></div>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a></h2>
<div class="section level3">
<h3 id="pre-requisites">Pre-requisites<a class="anchor" aria-label="anchor" href="#pre-requisites"></a></h3>
<ol style="list-style-type: decimal"><li>VAP environment and account
<ul><li>git (<code>sudo apt-get install git</code>)</li>
<li>docker CLI (<a href="https://gist.github.com/jkislin/a575780249c3580a472e25c07d3fd68f#file-docker_setup-sh" class="external-link">follow this script</a>). See the <a href="#appendix">appendix</a> if you wish to use podman instead for local runs</li>
<li>gh CLI (<code>sudo apt-get install gh</code>)</li>
<li>
<code>uv</code> command line tool. If not already installed, install following <a href="https://docs.astral.sh/uv/getting-started/installation/" class="external-link">these</a> instructions</li>
</ul></li>
<li>cfa-epinow2-pipeline repository in VAP
<ul><li>Navigate to where you would like to clone the repository code</li>
<li>Clone the repository (<code>git clone https://www.github.com/cdcgov/cfa-epinow2-pipeline</code> OR <code>gh auth login</code> and then <code>gh repo clone cdcgov/cfa-epinow2-pipeline</code>)</li>
</ul></li>
<li>Authentication to Azure. To authenticate to the requisite Azure resources provide a <code>.env</code> file containing the secrets necessary for authentication.
<ul><li>Request access to necessary Azure credential file (.env) from any of the admins listed in the README.md</li>
<li>decrypt the file (<code>gpg --decrypt .env</code>)</li>
<li>Place the decrypted file in your <code>cfa-epinow2-pipeline</code> directory</li>
</ul></li>
</ol></div>
<div class="section level3">
<h3 id="test-pre-requisites-are-setup">Test Pre-requisites are Setup<a class="anchor" aria-label="anchor" href="#test-pre-requisites-are-setup"></a></h3>
<div class="section level4">
<h4 id="test-configuration-generation">Test Configuration Generation<a class="anchor" aria-label="anchor" href="#test-configuration-generation"></a></h4>
<ol style="list-style-type: decimal"><li>
<code>make config</code> Running this command runs code located in the CDCgov/cfa-config-generator repository. This command creates a suite of configuration files, following the default settings for a production run, and saves them into <code>az://rt-epinow2-config/{job_id}</code>. If successful, something like this output will print to your command line. Make a note of the <code>job_id</code>, which you will need to identify your configs (and eventually your model outputs) in Azure blob storage.</li>
</ol><pre><code>make config
&gt; gh workflow run \
&gt;   -R cdcgov/cfa-config-generator run-workload.yaml \
&gt;   -f disease=all \
&gt;   -f state=all \
&gt;   -f job_id=Rt-estimation-20250318_1717845
&gt; Created workflow_dispatch event for `run_workload.yaml` at main</code></pre>
<p>Verify that you can locate the configuration files in Azure blob storage.</p>
<p>If you receive an error that you do not have the necessary permissions to run this command please reach out Nate McIntosh (<a href="mailto:ute2@cdc.gov">ute2@cdc.gov</a>) or Micah Wiesner (<a href="mailto:zqmk6@cdc.gov">zqmk6@cdc.gov</a>) for assistance.</p>
</div>
<div class="section level4">
<h4 id="test-make-run-command">Test make run command<a class="anchor" aria-label="anchor" href="#test-make-run-command"></a></h4>
<ol style="list-style-type: decimal"><li>
<p>The following command will test your setup for using the <code>CFAEpiNow2Pipeline</code> package. This command will run the pipeline for a single state and disease locally (using the computing power of your VAP account). This will take a few minutes. (N.B. the test config runs a short chain, and will be faster than a run using the default chain length.) <code>make run CONFIG=test/test.json</code></p>
<p><strong>Knowledge check:</strong> Where can you find the <code>test/test.json</code> config file?</p>
<details><summary><p>Hint</p>
</summary><p>Look somewhere in Azure blob storage.</p>
</details><details><summary><p>Answer</p>
</summary><p><code>az://rt-epinow2-config/test/</code>. This is the same pattern as in the config generation step, except that “test” is the job_id in this case.</p>
</details><p><br></p>
<p><strong>Knowledge check:</strong> Where can you find the outputs of a local run?</p>
<details><summary><p>Answer</p>
</summary><p><code>ls</code> should reveal two new local directories: <code>inputs/</code> contains a copy of the model inputs, and <code>job_id</code> contains the model outputs. These files are copied from the container to your local file system. Feel free to clean them up using <code>rm -r inputs</code> and <code>rm -r test</code>.</p>
</details><p><br></p>
</li>
<li>
<p>The following command will test your connection to Azure Batch resources. This command will run the pipeline for a single state (NY) and run using Azure Batch resources. To track the status of the nodes and pool, open Azure Batch Explorer. The outputs will write to the <code>nssp-rt-testing</code> container, in a directory labeled with the job_id printed to your command line. <code>make test-batch</code></p>
<p><strong>Knowledge check:</strong> Verify that you know how to track the jobs running in the batch explorer. Locate the outputs in Azure blob storage. Explore the contents, and open <code>tasks/{taks_id}/logs.txt</code> which is useful for debugging.</p>
<details><summary><p>Hint</p>
</summary><p>The outputs will be written to <code>az://nssp-rt-testing/{job_id}/</code></p>
</details></li>
</ol></div>
</div>
<div class="section level3">
<h3 id="rt-estimation-pipeline-production">Rt Estimation Pipeline (Production)<a class="anchor" aria-label="anchor" href="#rt-estimation-pipeline-production"></a></h3>
<p>If you have successfully setup the pre-requisites and are able to run <code>make config</code> and <code>make run CONGIF=test/test.json</code> you are ready to run the entire pipeline in production <code>make run-prod</code>. This command will run <code>make config</code>, then followed by a docker build, and then will the <code>job.py</code> script from in Batch; you only need to run <code>make run-prod</code> all of the work is done for you inside the Makefile! In doing so you are connecting to Azure Batch and setup 102 unique tasks that Azure Batch will run. This command is intended to close after initializing the jobs in Azure Batch. Please open Azure Batch Explorer to view the progress of these tasks.</p>
<div class="section level4">
<h4 id="exclusions-and-modifications">Exclusions and modifications<a class="anchor" aria-label="anchor" href="#exclusions-and-modifications"></a></h4>
<p>After reviewing the initial run, we may choose to make two kinds of changes to the models. First, we may exclude the results from the released predictions, if the data or the model do not pass quality checks. Second, we may exclude one or more data points from a specific state’s data, e.g. if there is evidence of a reporting anomaly on that date.</p>
<div class="section level5">
<h5 id="state-task-exclusions">State (task) Exclusions<a class="anchor" aria-label="anchor" href="#state-task-exclusions"></a></h5>
<p>Excluding a state from the released production files occurs downstream of this modeling pipeline, in the postprocessing step. Even if a state was run, and results were generated, the postprocessor can be configured to skip over the excluded jurisdictions when collecting the modeling results.</p>
<p>To exclude a state from the initial model run (e.g. if no data were received), generate appropriate configs. Task exclusions are state and disease pairs (ex. ‘NY:COVID-19)’ that users can specify when running the config generator so that tasks for specified state disease pairs are not generated unnecessarily. Users can specify more than one state and disease pair, so long as they are separate by a ‘,’ (ex. ‘NY:COVID-19,WA:Influenza’). To do so, a user will need to first, generate the configuration files and specify the state:disease pair to exclude (see example below) and then run the pipeline for these configurations.</p>
<ol style="list-style-type: decimal"><li><code>TIMESTAMP=$(date -u +"%Y%m%d_%H%M%S"); JOB_ID=Rt-estimation-$(echo $TIMESTAMP)</code></li>
<li><code>gh workflow run -R cdcgov/cfa-config-generator run-workload.yaml -f job_id=$(echo $JOB_ID) -f task_exclusions='NY:COVID-19', -f output_container='nssp-rt-v2'</code></li>
<li><code>make run-batch JOB=$(echo $JOB_ID)</code></li>
</ol><p>Note: the <code>JOB=$(echo $JOB_ID)</code> option in step 3. instructs the model run to use the config files generated in steps 1. and 2. Without this option, <code>make run-batch</code> will generate new configs using the default settings, and will run using those.</p>
</div>
<div class="section level5">
<h5 id="data-exclusions">Data Exclusions<a class="anchor" aria-label="anchor" href="#data-exclusions"></a></h5>
<p>During the anomaly review meeting, we sometimes deem some points anomalous; they are affecting the model in some undesired way. The result of this is that we usually want to re-run a state-disease pairs. So how do we re-run those state-disease pairs, and tell the pipeline to drop certain data points?</p>
<p>The meeting attendees will fill in a spreadsheet which specifies which state-disease pairs will be rerun, and what specific data points will be removed. For the pipeline to know which data points to exclude when it loads the data, it needs to be pointed to a data “outliers” (a.k.a. “exclusions”) CSV in its configuration file. So to get from the spreadsheet of decisions to running those particular cases again in Azure Batch, we use the following flow.</p>
<pre class="mermaid"><code>flowchart TD
    A(Data Anomaly Review) --&gt;|produces| B(Decisions Spreadhsheet)
    B --&gt;|Run script on docker to format and upload outlier file| C(YYYY-MM-DD.csv in utils/Rt_review_exclusions.R)
    C --&gt;|Manually check file located in...| D(YYYY-MM-DD.csv in Azure Blob: az://nssp-etl/outliers-v2/YYYY-MM-DD.csv)
    D --&gt;|make rerun-prod| E(New set of configs)
    E --&gt;F(Run those tasks in Azure Batch)</code></pre>
<p>In this flowchart, each arrow with text represents a manual action. Writing out the above in more detail: 1. The anomaly review team documents their decisions in the spreadsheet 1. The person running the pipeline pulls an updated docker using <code>make pull</code> and open’s the docker with <code>make up</code> 1. Then the user enters <code>cd cfa-epinow2-pipeline/utils</code> into the terminal as well as <code>Rscript Rt_review_exclusions.R -d yyymmdd</code> where yyyymmdd is the date in the name of the Rt_review_yyymmdd.xlsx excel file on sharepoint. The -d argument’s default is today’s date 1. The terminal will prompt you to login using a provided url and a code. Copy the url and paste into a browser where you are logged onto your CDC account (not ext account). Then paste the provided code. Then confirm your login by hitting continue. Once confirmed, the script will download the sharepoint excel file, process it, and upload it as the outlier csv file to the blob storage “folder” <a href="az://nssp-etl/outliers-v2/"><code>az://nssp-etl/outliers-v2/</code></a> 1. The person running the pipeline runs <code>make rerun-prod</code>. This will create new configuration files that include the path to the outlier CSV just uploaded to Blob, and then kick off those tasks in Azure Batch</p>
<p>The outlier file will have these columns: <code>state</code>, <code>disease</code>, <code>reference_date</code>, <code>report_date</code>. Those columns fully specify for the pipeline how to handle the data exclusions. Note that this set of data outliers corresponds to the NSSP report in use for this production run. If we ran again tomorrow, we would use tomorrow’s report, the data would be different, and we would likely pick a different set of points (if any) to be marked as outliers. This means it is important to always use the date of this report as the name for this CSV. (It should correspond with the values in the <code>report_date</code> column).</p>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="non-production-runs">Non-production runs<a class="anchor" aria-label="anchor" href="#non-production-runs"></a></h2>
<p>The config file uniquely defines a run. To kick off a non-production run, test, or experiment, generate an appropriate set of configs.</p>
<p><strong>DO NOT</strong> Write tests or experimental output to the production container, <code>nssp-rt-v2</code>. Instead, please direct non-production output to <code>nssp-rt-testing</code>. The testing container is the default output container in the config generator.</p>
<p>There are three ways to generate configs:</p>
<ol style="list-style-type: decimal"><li>Use the GUI in the config generation repo Navigate to <a href="https://github.com/CDCgov/cfa-config-generator/actions/workflows/run-workload.yaml" class="external-link uri">https://github.com/CDCgov/cfa-config-generator/actions/workflows/run-workload.yaml</a>, click the “Run workflow” dropdown menu, specify your parameters, and hit “Run workflow”. This will write a set of configs to <code>az://rt-epinow2-config/{job_id}</code>. Then run <code>make run-batch JOB=job_id</code> to run the models using the configs you just generated.</li>
</ol><p><img src="image.png" alt="screenshot of UI"> 2. Use the command line <code>gh workflow run -R cdcgov/cfa-config-generator run-workload.yaml -f job_id='my_job_id' -f task_exclusions='NY:COVID-19', -f output_container='nssp-rt-testing'</code> Use -f flags to specify arguments different from the defaults</p>
<ol start="3" style="list-style-type: decimal"><li>(We recommend you do NOT) run <code>make config</code>. This will specify the production container, <code>nssp-rt-v2</code>, as the output destination. We don’t want to contaminate the production output with test runs and experiments. Use this for production runs only.</li>
</ol><p>Note that if you need to modify a config, you may download it from blob, edit, and re-upload to overwrite the copy in blob.</p>
<div class="section level3">
<h3 id="appendix">Appendix<a class="anchor" aria-label="anchor" href="#appendix"></a></h3>
<div class="section level4">
<h4 id="podman">Podman<a class="anchor" aria-label="anchor" href="#podman"></a></h4>
<p>The default container management software is setup to utilize docker. For users that are currently using podman, please adjust the variable <code>CNTR_MGR</code> within the makefile prior to running any commands. Further, it will be necessary to authenticate to azure resources through podman (<code>podman login</code>).</p>
</div>
</div>
</div>


  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Zachary Susswein.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

