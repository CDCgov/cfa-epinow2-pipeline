# Introduction to EpiNow2 pipeline 2.0

## Goals

1. Show what we've accomplished thus far
2. Talk about what's left to do
3. Give some hands on experience

## What have we done?

In many ways, this pipeline is similar in spirit to our existing pipeline.
It fits a model in EpiNow2 and saves some outputs to a storage container.

But it makes a lot of annoying things easier.

Lets talk it through on my work-in-progress branch:
```sh
gh repo clone cdcgov/cfa-epinow2-pipeline
gh pr checkout 106
```

## Maintaining Docker images

>[!NOTE]
> This section was masterminded by George. He did a fabulous job and I've already found it a huge quality-of-life increase.

We don't need to do (long, slow) local builds anymore.
GitHub is set up to rebuild the Docker image when you open a PR and on merge to main.
GitHub maintains its own cache, so this build should only ever take a minute or two unless there's a change in the base image.
In that case it currently takes ~10 mins, but that's on GitHub's time and you don't need to worry about it.

After building the image, GitHub automatically tags it and pushes it to ACR.
If you have our NNH CLI installed, you can see all the tagged images in ACR with
```sh
nnh list-acr-image-tags cfa-epinow2-pipeline
```

You can pull the built image down locally for prototyping with
```sh
make pull
```

If you `make pull`, it will pull the image associated with your branch (if there's an open PR for that branch).

I've been prepping for today on the `edit-azure-flow` branch.
Try pulling that image:
```sh
git pull
git switch edit-azure-flow
make pull
```

This command should handle all the login for you and set you up with a local copy of the image we build in GitHub.

If you want to look in the image environment, you can run `make up` to get dropped into shell of a loaded, working container with everything set up.

If you want to do some local iteration, you can easily rebuild everything with `make build`. This command handles all the steps and, if you run it after an initial `make pull`, takes advantage of the cached image to keep things quick.

## Azure Batch

We also automatically set up an associated Batch pool linked to the image.
We refresh the pool every time we refresh the image.

Basically, we can stop worrying about managing pools and spend more time thinking about model performance.

## Managing configs

>[!NOTE]
> This bit was masterminded by Agastya. He implemented some incredibly cool stuff and I learned quite a bit watching him. Go check out the code when you have a free minute.

We can use a nice button or a script-based workflow to generate configs and drop them into a dedicated `rt-epinow2-config` storage container.

[See here](https://github.com/CDCgov/cfa-config-generator/actions/workflows/run-workload.yaml)

Or try running this command
```sh
gh workflow run \
  -R cdcgov/cfa-config-generator run-workload.yaml  \
  -f disease=all \
  -f state=all \
  -f reference_dates=2024-01-01,2024-10-10 \
  -f report_date=2024-11-12
```

This means no more hand editing configs! And please don't hand-edit them.
We want to use new unique IDs for every run, which this service automatically handles for us.

## Pipeline development

>[!NOTE]
> Adam, Katie, and Nate contributed extensively here.

We've moved the modeling code into a package and added extensive testing in CI.
We've also added quite a few runtime checks, to prevent regressions from past bugs (e.g., GI formatting).

These changes should make the code safer and easier to update.
You get immediate feedback on your changes before anything gets close to production.

These changes won't eliminate bugs, but hopefully it will help.


## Updated code

The code now has some more guardrails and automated checks.
We use the REST API to pull files up and down, so no more worrying about blobfuse caching.
It points to Adam's new parameter estimates and checks that the results match its expected format.
It generates the summaries and draws we want and saves them in parquet for easy exploration (no more munging the Stan object!).

## Running the pipeline

Let's give it a local test.
Automatically pointing the pipeline to a newly generated config is still a poin point, so for today let's use a pre-generated configuration file.

First, I'm going to send everyone the credentials. (Zack -- send people creds).

The command to run things is
```sh
make run CONFIG='test.json'
```

The pipeline will automatically pull down this config and run it.
In the future we could use this approach for local debugging of unexpected pipeline failures.
When run locally, the pipeline will save outputs onto your local directory.
It will also save everything in the storage container (in this case `zs-test-pipeline-update`).

## Running in Batch

I only got this running this morning......so let's give it a shot.

Give this a go:
```sh
make run-batch JOB='<your-initials>`
```

If all goes well, this should submit a batch job. Go check Batch Explorer and let me know what you see.

## Known pain points

* Credentials: A local run requires credentials to be set up locally (like the `configuration.toml` in the current pipeline
    * Not ideal!
    * I don't have a workaround right now
    * But we're working toward making GitHub handle the Azure bits, so non-local runs should all be automatic and easy.
* The docs are still messy and need some love.

## Big things left to do

* Stitching services together:
   * Config generation -> validation
   * Config -> pipeline run
   * Job kickoff
* Some live fire testing
   * Things will break
   * Some tasks will be harder or annoying
   * I want to hear about them. We will make things better.
* More automation of jobs
